<div align="center">

<img src="https://huggingface.co/datasets/optimum/documentation-images/resolve/main/executorch/logo/optimum-executorch.png" width=80%>

# ü§ó Optimum ExecuTorch

**Optimize and deploy Hugging Face models with ExecuTorch**

[Documentation](https://huggingface.co/docs/optimum/index) | [ExecuTorch](https://github.com/pytorch/executorch) | [Hugging Face](https://huggingface.co/)

</div>

## üöÄ Overview

Optimum ExecuTorch enables efficient deployment of transformer models using Meta's ExecuTorch framework. It provides:
- üîÑ Easy conversion of Hugging Face models to ExecuTorch format
- ‚ö° Optimized inference with hardware-specific optimizations
- ü§ù Seamless integration with Hugging Face Transformers
- üì± Efficient deployment on various devices

## ‚ö° Quick Installation

Install from source:
```
git clone https://github.com/huggingface/optimum-executorch.git
cd optimum-executorch
pip install .
```

- üîú Install from pypi coming soon...

## üéØ Quick Start

There are two ways to use Optimum ExecuTorch:

### Option 1: Export and Load Separately

#### Step 1: Export your model
Use the CLI tool to convert your model to ExecuTorch format:
```
optimum-cli export executorch \
    --model "meta-llama/Llama-3.2-1B" \
    --task "text-generation" \
    --recipe "xnnpack" \
    --output_dir="meta_llama3_2_1b"
```

#### Step 2: Load and run inference
Use the exported model for text generation:
```python
from optimum.executorch import ExecuTorchModelForCausalLM
from transformers import AutoTokenizer

# Load the exported model
model = ExecuTorchModelForCausalLM.from_pretrained(
    "./meta_llama3_2_1b",
    export=False
)

# Initialize tokenizer and generate text
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B")
generated_text = model.text_generation(
    tokenizer=tokenizer,
    prompt="Simply put, the theory of relativity states that",
    max_seq_len=128
)
```

### Option 2: Python API
```python
from optimum.executorch import ExecuTorchModelForCausalLM
from transformers import AutoTokenizer

# Load and export model in one step
model = ExecuTorchModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-1B",
    export=True,
    recipe="xnnpack"
)

# Generate text right away
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B")
generated_text = model.text_generation(
    tokenizer=tokenizer,
    prompt="Simply put, the theory of relativity states that",
    max_seq_len=128
)
```

## üõ†Ô∏è Advanced Usage

Check our [ExecuTorch GitHub repo](https://github.com/pytorch/executorch) directly for:
- Custom model export configurations
- Performance optimization guides
- Deployment guides for Android, iOS, and embedded devices
- Additional examples

## ü§ù Contributing

We love your input! We want to make contributing to Optimum ExecuTorch as easy and transparent as possible. Check out our:

- [Contributing Guidelines](CONTRIBUTING.md)
- [Code of Conduct](CODE_OF_CONDUCT.md)

## üìù License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## üì´ Get in Touch

- Report bugs through [GitHub Issues](https://github.com/huggingface/optimum-executorch/issues)
