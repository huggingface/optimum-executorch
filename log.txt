Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  3.66it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.77it/s]
`torch_dtype` is deprecated! Use `dtype` instead!
WARNING:coremltools:scikit-learn version 1.7.1 is not supported. Minimum required version: 0.17. Maximum required version: 1.5.1. Disabling scikit-learn conversion API.
WARNING:coremltools:Torch version 2.10.0.dev20251003+cu126 has not been tested with coremltools. You may run into unexpected errors. Torch 2.7.0 is the most recent version that has been tested.
WARNING:coremltools:Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'
WARNING:coremltools:Failed to load _MLCPUComputeDeviceProxy: No module named 'coremltools.libcoremlpython'
WARNING:coremltools:Failed to load _MLGPUComputeDeviceProxy: No module named 'coremltools.libcoremlpython'
WARNING:coremltools:Failed to load _MLNeuralEngineComputeDeviceProxy: No module named 'coremltools.libcoremlpython'
WARNING:coremltools:Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'
WARNING:coremltools:Failed to load _MLComputePlanProxy: No module named 'coremltools.libcoremlpython'
WARNING:coremltools:Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'
WARNING:coremltools:Failed to load _MLModelAssetProxy: No module named 'coremltools.libcoremlpython'
I tokenizers:regex.cpp:27] Registering override fallback regex
/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1824: UserWarning: While exporting, we found certain side effects happened in the model.forward. Here are the list of potential sources you can double check: ["L['self'].cache.layers[0]", "L['self'].cache.layers[1]", "L['self'].cache.layers[2]", "L['self'].cache.layers[3]", "L['self'].cache.layers[4]", "L['self'].cache.layers[6]", "L['self'].cache.layers[7]", "L['self'].cache.layers[8]", "L['self'].cache.layers[9]", "L['self'].cache.layers[10]", "L['self'].cache.layers[12]", "L['self'].cache.layers[13]", "L['self'].cache.layers[14]", "L['self'].cache.layers[15]", "L['self'].cache.layers[16]", "L['self'].cache.layers[18]", "L['self'].cache.layers[19]", "L['self'].cache.layers[20]", "L['self'].cache.layers[21]", "L['self'].cache.layers[22]", "L['self'].cache.layers[24]", "L['self'].cache.layers[25]", "L['self'].cache.layers[26]", "L['self'].cache.layers[27]", "L['self'].cache.layers[28]", "L['self'].cache.layers[30]", "L['self'].cache.layers[31]", "L['self'].cache.layers[32]", "L['self'].cache.layers[33]"]
  warnings.warn(
Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 5753.50it/s]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  return torch._C._get_cublas_allow_tf32()
/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:319: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Traceback (most recent call last):
  File "/home/gasoonjia/.conda/envs/et/bin/optimum-cli", line 7, in <module>
    sys.exit(main())
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/optimum/commands/optimum_cli.py", line 208, in main
    service.run()
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/optimum/commands/export/executorch.py", line 181, in run
    main_export(
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/optimum/exporters/executorch/__main__.py", line 145, in main_export
    return export_to_executorch(
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/optimum/exporters/executorch/convert.py", line 80, in export_to_executorch
    executorch_progs = recipe_func(model, **kwargs)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/optimum/exporters/executorch/recipes/cuda.py", line 139, in export_to_executorch_with_cuda
    return _lower_to_executorch(exported_progs, model.metadata)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/optimum/exporters/executorch/recipes/cuda.py", line 97, in _lower_to_executorch
    et_prog = to_edge_transform_and_lower(
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/executorch/exir/program/_program.py", line 115, in wrapper
    return func(*args, **kwargs)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/executorch/exir/program/_program.py", line 1375, in to_edge_transform_and_lower
    edge_manager = edge_manager.to_backend(method_to_partitioner)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/executorch/exir/program/_program.py", line 115, in wrapper
    return func(*args, **kwargs)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/executorch/exir/program/_program.py", line 1677, in to_backend
    new_edge_programs = to_backend(method_to_programs_and_partitioners)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/functools.py", line 889, in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/executorch/exir/backend/backend_api.py", line 762, in _
    lower_all_submodules_to_backend(
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/executorch/exir/backend/backend_api.py", line 591, in lower_all_submodules_to_backend
    backend_name_to_subclass[backend_id].preprocess_multimethod(
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/executorch/exir/backend/backend_details.py", line 129, in preprocess_multimethod
    preprocess_result = cls.preprocess(program, compile_spec_for_program)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/executorch/backends/cuda/cuda_backend.py", line 165, in preprocess
    so_path = torch._inductor.aot_compile(edge_program_module, tuple(user_input_placeholders), options=options)  # type: ignore[arg-type]
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/__init__.py", line 310, in aot_compile
    return compile_fx_aot(
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1949, in compile_fx_aot
    compiled_artifacts = compile_fx(
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 2414, in compile_fx
    return compile_fx(
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 2453, in compile_fx
    return _maybe_wrap_and_compile_fx_main(
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 2539, in _maybe_wrap_and_compile_fx_main
    return _compile_fx_main(
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 2717, in _compile_fx_main
    return inference_compiler(unlifted_gm, example_inputs_)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/schemas.py", line 1242, in __call__
    return self.compiler_fn(gm, example_inputs)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 2603, in fw_compiler_base
    return compile_fx_forward(
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 2283, in compile_fx_forward
    return inner_compile(
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 792, in compile_fx_inner
    return wrap_compiler_debug(_compile_fx_inner, compiler_name="inductor")(
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 144, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 973, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1703, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1472, in codegen_and_compile
    wrapper_code, kernel_code = graph.codegen_with_cpp_wrapper()
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2260, in codegen_with_cpp_wrapper
    return self.codegen()
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2300, in codegen
    self._update_scheduler()
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2294, in _update_scheduler
    self.scheduler = Scheduler(self.operations)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 2254, in __init__
    self._init(nodes)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 2336, in _init
    self.nodes = self.fuse_nodes(self.nodes)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 3024, in fuse_nodes
    nodes = self.fuse_nodes_once(nodes, is_reorder_round=True)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 3597, in fuse_nodes_once
    for node1, node2 in self.get_possible_fusions(nodes, is_reorder_round):
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 3728, in get_possible_fusions
    check_all_pairs(node_grouping)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 3713, in check_all_pairs
    if self.can_fuse(node1, node2, is_reorder_round):
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 4312, in can_fuse
    new_shared_data_score = self.shared_data_after_reordering_loop(node1, node2)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 4007, in shared_data_after_reordering_loop
    reordered = node1.reorder_loops_by_dep_pair(lhs_dep, rhs_dep)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 1302, in reorder_loops_by_dep_pair
    self.apply_new_loop_order(new_order)
  File "/home/gasoonjia/.conda/envs/et/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 1253, in apply_new_loop_order
    self._body = self._body.reorder_iter_loops(
AttributeError: 'NoneType' object has no attribute 'reorder_iter_loops'
